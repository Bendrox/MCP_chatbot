{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25968da",
   "metadata": {},
   "source": [
    "## Testing MCP server (local-fake remote) with OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6acd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Statistics ------\n",
      "Input tokens : 945168\n",
      "Output tokens : 8290\n",
      "Total tokens : 953458\n",
      "\n",
      "\n",
      "------------------  Chatbot reponse  ------------------\n",
      "Voici une proposition de README professionnel pour le dépôt MCP_chatbot, basée exclusivement sur l’analyse de la structure et des fichiers du projet (en ignorant le README existant). Je l’ai également sauvegardée sous le chemin: /workdir_filesystem/README_PROPOSITION.md. Dites-moi si vous souhaitez que je crée une branche et une Pull Request pour remplacer le README.md du repo.\n",
      "\n",
      "Titre\n",
      "MCP Chatbot — Chatbot multi-outils via Model Context Protocol (MCP)\n",
      "\n",
      "Description\n",
      "Projet Python offrant une série de chatbots (LvL1 → LvL4) capables d’utiliser des outils locaux et des serveurs MCP (Model Context Protocol) pour enrichir leurs réponses. Les versions avancées se connectent à plusieurs serveurs (filesystem, fetch, git, research) et exposent également des ressources (@folders, @topic) et des prompts réutilisables.\n",
      "\n",
      "Points clés\n",
      "- Multi-niveaux: du chatbot local (arXiv) au multi-serveurs MCP avec prompts/resources\n",
      "- LLMs supportés: Anthropic Claude (par défaut), exemples OpenAI fournis\n",
      "- Outils arXiv prêts à l’emploi: search_papers, extract_info\n",
      "- Config JSON incluse pour lancer les serveurs MCP\n",
      "- Notebooks d’exemples d’intégration MCP\n",
      "\n",
      "1) Architecture du projet\n",
      "- lanceur.py: menu CLI pour choisir la version LvL1 à LvL4\n",
      "- chatbots/\n",
      "  - chatbot_lvl1_tools.py: Chatbot local avec outils Python (arXiv)\n",
      "  - chatbot_lvl2_mcp_tools.py: Chatbot + 1 serveur MCP “research”\n",
      "  - chatbot_lvl3_multi_mcp.py: Chatbot multi-serveurs MCP (filesystem, fetch, git, research)\n",
      "  - chatbot_lvl4.py: Comme LvL3 + gestion des prompts et resources MCP\n",
      "- chatbots_openai/\n",
      "  - chatbot_lvl3.py: Variante LvL3 utilisant OpenAI\n",
      "- chatbot_lvl1_tools/\n",
      "  - arxiv_tools_4_chatbot.py: implémentations locales des outils arXiv\n",
      "- local_mcp_servers/\n",
      "  - mcp_server_research.py: serveur MCP “research” (outils arXiv)\n",
      "  - mcp_server_research_lvl4.py: idem + resources (papers://...) + prompt “generate_search_prompt”\n",
      "- mcp_server_config/\n",
      "  - mcp_server_config.json: config des serveurs MCP (LvL2/LvL3)\n",
      "  - mcp_server_config_lvl4.json: config pour LvL4 (incluant resources/prompts)\n",
      "- llm/\n",
      "  - claude_models.py: wrappers Anthropic (Claude 3.5/4)\n",
      "  - openai_models.py: wrappers OpenAI (exemples)\n",
      "- notebooks/: explore.ipynb, openai_mcp_chat.ipynb\n",
      "- chatbot_ouputs/: exemples de sorties (EN/FR)\n",
      "- docker-compose.yml + dockerfile: proxy SSE + serveur filesystem MCP (optionnel)\n",
      "- requirements.txt: dépendances\n",
      "\n",
      "2) Prérequis\n",
      "- Python 3.10+\n",
      "- Node.js 18+ (pour npx des serveurs MCP) ou Docker\n",
      "- Variables d’environnement selon le LLM:\n",
      "  - ANTHROPIC_API_KEY (Claude)\n",
      "  - OPENAI_API_KEY (si variante OpenAI)\n",
      "\n",
      "Exemple .env\n",
      "ANTHROPIC_API_KEY=sk-ant-…\n",
      "OPENAI_API_KEY=sk-…\n",
      "\n",
      "3) Installation\n",
      "- git clone https://github.com/Bendrox/MCP_chatbot.git\n",
      "- cd MCP_chatbot\n",
      "- python -m venv .venv && source .venv/bin/activate  (Windows: .venv\\Scripts\\activate)\n",
      "- pip install -r requirements.txt\n",
      "- Créez un .env à la racine (voir ci-dessus)\n",
      "\n",
      "Option Docker (serveur filesystem via SSE proxy)\n",
      "- docker compose up\n",
      "- SSE exposé sur http://localhost:8000/sse (utile pour consommation HTTP/SSE)\n",
      "\n",
      "4) Configuration des serveurs MCP\n",
      "Les fichiers JSON définissent les serveurs lancés par le client (stdio) côté chatbot:\n",
      "- mcp_server_config/mcp_server_config.json\n",
      "- mcp_server_config/mcp_server_config_lvl4.json\n",
      "\n",
      "Exemples:\n",
      "- filesystem: npx -y @modelcontextprotocol/server-filesystem .\n",
      "- research: python3 local_mcp_servers/mcp_server_research.py\n",
      "- fetch: python3 -m mcp_server_fetch\n",
      "- git: python3 -m mcp_server_git\n",
      "\n",
      "Important\n",
      "- Dans chatbots/chatbot_lvl2_mcp_tools.py, chatbots/chatbot_lvl3_multi_mcp.py et chatbots/chatbot_lvl4.py, remplacez les chemins absolus d’exemple vers le fichier JSON de config par un chemin relatif du dépôt (ex.: mcp_server_config/mcp_server_config.json).\n",
      "\n",
      "5) Lancer le chatbot\n",
      "- Menu interactif:\n",
      "  - python lanceur.py (choisir 1, 2, 3, 4)\n",
      "\n",
      "- Exécuter un module:\n",
      "  - LvL1: python -m chatbots.chatbot_lvl1_tools\n",
      "  - LvL2: python -m chatbots.chatbot_lvl2_mcp_tools\n",
      "  - LvL3: python -m chatbots.chatbot_lvl3_multi_mcp\n",
      "  - LvL4: python -m chatbots.chatbot_lvl4\n",
      "  - Variante OpenAI (LvL3): python -m chatbots_openai.chatbot_lvl3\n",
      "\n",
      "6) Fonctionnement par niveau\n",
      "- LvL1 — Outils locaux (sans MCP)\n",
      "  - Outils: search_papers(topic, max_results), extract_info(paper_id)\n",
      "  - Flux: le LLM choisit un outil → exécution Python locale → retour au modèle → réponse finale\n",
      "\n",
      "- LvL2 — 1 serveur MCP (research)\n",
      "  - Démarre un serveur MCP local (stdio)\n",
      "  - Le LLM appelle les outils exposés par le serveur\n",
      "\n",
      "- LvL3 — Multi-MCP\n",
      "  - Connexion à filesystem, fetch, git, research\n",
      "  - Mapping outil → session: chaque tool est routé vers le bon serveur\n",
      "\n",
      "- LvL4 — Prompts et Resources MCP\n",
      "  - En plus des tools: listage/usage de prompts et resources\n",
      "  - Resources (via mcp_server_research_lvl4):\n",
      "    - papers://folders → liste des sujets disponibles\n",
      "    - papers://{topic} → détail des papiers d’un sujet\n",
      "  - Commandes:\n",
      "    - @folders → affiche les dossiers\n",
      "    - @{topic} → affiche les papiers d’un topic\n",
      "    - /prompts → liste les prompts\n",
      "    - /prompt <nom> arg1=val1 … → exécuter un prompt puis poursuivre la conversation\n",
      "\n",
      "7) Détails LLM\n",
      "- Par défaut, les scripts utilisent Claude35 (Anthropic)\n",
      "- Wrappers: llm/claude_models.py et llm/openai_models.py\n",
      "- Vous pouvez basculer vers Claude4 ou la variante OpenAI\n",
      "\n",
      "8) Données et sorties\n",
      "- papers/: créé à l’exécution pour stocker les métadonnées arXiv par sujet\n",
      "- chatbot_ouputs/: exemples de synthèses générées (EN/FR)\n",
      "\n",
      "9) Dépannage\n",
      "- Erreur d’authentification/modèle: vérifiez ANTHROPIC_API_KEY / OPENAI_API_KEY\n",
      "- npx introuvable: installez Node 18+ ou utilisez Docker\n",
      "- Outils MCP vides: vérifiez que les serveurs démarrent (droits, Python/Node)\n",
      "- Chemins absolus: remplacez-les par des chemins relatifs vers mcp_server_config/*.json\n",
      "\n",
      "10) Roadmap / limites connues\n",
      "- Remplacer les chemins absolus par une variable d’environnement (ex.: MCP_CONFIG_FILE)\n",
      "- Adapter les noms de modèles OpenAI aux versions disponibles\n",
      "- Ajouter des tests et une CI\n",
      "- Exemple client consommant un serveur MCP distant via SSE/HTTP\n",
      "\n",
      "11) Contribution\n",
      "- Issues/PRs bienvenues. Merci d’isoler les changements par niveau (LvL1→LvL4).\n",
      "\n",
      "12) Licence\n",
      "- Aucune licence explicite détectée. Ajoutez un fichier LICENSE si vous souhaitez ouvrir la redistribution/contribution.\n",
      "\n",
      "Souhaitez-vous que je crée une branche (ex.: docs/pro-readme) et une Pull Request pour remplacer README.md dans le dépôt GitHub, ou préférez-vous l’ajouter vous-même à partir du fichier sauvegardé ?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "load_dotenv()\n",
    "\n",
    "def get_token():\n",
    "    token_url = 'https://oauth.piste.gouv.fr/api/oauth/token'\n",
    "    #inject cred \n",
    "    token_data = {\n",
    "    'grant_type': 'client_credentials',\n",
    "    'client_id': client_id_prod,\n",
    "    'client_secret': client_secret_prod,\n",
    "    'scope': 'openid'}\n",
    "    response = requests.post(token_url, data=token_data)\n",
    "    response.raise_for_status()  # vérif  erreurs\n",
    "    # récup  jeton\n",
    "    token_info = response.json()\n",
    "    access_token = token_info['access_token']\n",
    "    return access_token\n",
    "\n",
    "#loading tokens\n",
    "github_barer=os.getenv('github_barer')\n",
    "client_id_prod=os.getenv('client_id_prod')\n",
    "client_secret_prod=os.getenv('client_secret_prod')\n",
    "\n",
    "## loading MCP file configuration\n",
    "with open(\"/Users/oussa/Desktop/Github_perso/chatbot_mcp/mcp_server_config/mcp_server_config_openai.json\", \"r\") as file:\n",
    "    mcp_server_config_data = json.load(file)\n",
    "                \n",
    "## inject github token \n",
    "for s in mcp_server_config_data:\n",
    "    if s[\"server_label\"] == \"github\":\n",
    "        s[\"headers\"][\"Authorization\"] = f\"Bearer {github_barer}\"\n",
    "   \n",
    "## get LegiFrance token \n",
    "token=get_token()\n",
    "\n",
    "resp = OpenAI().responses.create(\n",
    "    #model=\"gpt-5-nano-2025-08-07\",\n",
    "    model='gpt-5-mini-2025-08-07',\n",
    "    #model='gpt-5-2025-08-07',\n",
    "    tools= mcp_server_config_data,\n",
    "#input=\"clone ce repos https://github.com/Bendrox/MCP_chatbot.git et fait moi une proposition de fichier README professionel en analysant les fichiers et la structure du projet et sauvgarde le résultat en faisant abstraction du fichier README deja dans le projet \"\n",
    "input=\"Sans appeler d'outil, dresse un inventaire des outils MCP que tu vois\"\n",
    "#input=f\"récupère et compare les versions précédente et en vigueur du texte LEGIARTI000041577698 et trouve moi le numéro celex de la directive a l origine de la modification tu pourras utiliser le token suivant {token}  pour récupérer les infos \"\n",
    ")\n",
    "\n",
    "print(\"------ Statistics ------\")\n",
    "print(f\"Input tokens : {resp.usage.input_tokens}\" )\n",
    "print(f\"Output tokens : {resp.usage.output_tokens}\" )\n",
    "#print(f\"cached_tokens : {resp.usage.input_tokens_details.cached_tokens}\")\n",
    "print(f\"Total tokens : {resp.usage.total_tokens}\" )\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"------------------  Chatbot reponse  ------------------\")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
