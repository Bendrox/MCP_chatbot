# ğŸ¤– MCP Chatbot: Multiâ€‘Level Assistant

## ğŸ§­ Overview

This repository contains a set of multiâ€‘tool chatbots (LvL1 â†’ LvL4) powered by OpenAI and/or Claude (Anthropic), enhanced with the Model Context Protocol (MCP). The goal is to provide several assistants and addâ€‘ons that enable:

- Retrieval and analysis of publications (arXiv)
- Synthesis and organization of scientific content
- Access to local resources (file system) and remote resources (Git repositories, HTTP fetch)
- Integration of local and remote MCP servers to extend capabilities (prompts, resources)
- A homemade MCP server to retrieve data from **LÃ©giFrance**

## ğŸ¯ Goal

Provide a modular platform for experimenting with specialized searchâ€‘based conversational assistants capable of orchestrating multiple models and tools via MCP and automating tasks related to the collection, summarization, and management of legal and scientific content.

## ğŸ§© Chatbot Levels 

- **chatbot\_lvl1** â€” Basic chatbot with local tools to interact with arXiv.
- **chatbot\_lvl2** â€” Enhanced chatbot with a single MCP server (arXiv tools via â€œresearchâ€).
- **chatbot\_lvl3** â€” Advanced chatbot using multiple MCP servers (filesystem, fetch, Git, research).
- **chatbot\_lvl4** â€” Advanced multiâ€‘model chatbot using external MCP servers, plus prompts and local resources.

## ğŸ“ Repository Structure (topâ€‘level)

- `chatbot_lvl1_tools/` â€” Basic tools for the chatbot (e.g., arXiv interaction).
- `chatbot_ouputs/` â€” (probable) directory for storing chatbot outputs.
- `chatbots/` â€” Chatbot implementations (different levels/variants) with Claude (Anthropic).
- `chatbots_openai/` â€” Specific integrations and agents with OpenAIâ€™s GPTâ€‘5.
- `llm/` â€” Wrappers and utilities for LLMs (pre/postâ€‘processing, prompts).
- `local_mcp_servers/` â€” Configurations/launchers for local MCP servers (filesystem, research, Gitâ€¦).
- `mcp_server_config/` â€” Configuration files for MCP servers in use.
- `notebooks/` â€” Jupyter notebooks for exploration and demos.
- `retreived_arxiv_papers/` â€” Collection of papers retrieved for testing/examples.
- `requirements.txt` â€” Python dependencies.
- `docker-compose.yml` + `Dockerfile` â€” SSE proxy + filesystem MCP server (for OpenAI filesystem).

## ğŸ—ï¸ Project Architecture

- `lanceur.py` â€” CLI launcher to run a chatbot version (LvL1 â†’ LvL4)
- `chatbots/`
  - `chatbot_lvl1_tools.py` â€” Local chatbot with Python tools (arXiv)
  - `chatbot_lvl2_mcp_tools.py` â€” Chatbot + 1 MCP server (research)
  - `chatbot_lvl3_multi_mcp.py` â€” Chatbot with multiple MCP servers (filesystem, fetch, Git, research)
  - `chatbot_lvl4.py` â€” As LvL3, with MCP prompts and resources management
- `chatbots_openai/`
  - `chatbot_lvl3.py` â€” LvL3 variant using OpenAI APIs
- `chatbot_lvl1_tools/`
  - `arxiv_tools_4_chatbot.py` â€” Local implementations of arXiv tools
- `local_mcp_servers/`
  - `mcp_server_research.py` â€” MCP â€œresearchâ€ server (arXiv tools)
  - `mcp_server_research_lvl4.py` â€” Same + resources (`papers://â€¦`) + `generate_search_prompt` prompt
- `mcp_server_config/`
  - `mcp_server_config.json` â€” MCP server config for LvL2/LvL3
  - `mcp_server_config_lvl4.json` â€” Config for LvL4 (including resources/prompts)
- `llm/`
  - `claude_models.py` â€” Anthropic wrappers (Claude 3.5/4)
  - `openai_models.py` â€” OpenAI wrappers (GPTâ€‘5 / 5â€‘nano / 5â€‘mini)
- `notebooks/`
  - `explore.ipynb`, `openai_mcp_chat.ipynb` â€” Demonstrations
- `chatbot_ouputs/` â€” Example outputs (MCP summaries)

## ğŸ”Œ MCP Servers & Integrations

- **ArXiv** (in `arxiv_tools_4_chatbot.py`): retrieve and analyze research papers; search and extract information from academic repositories; support inâ€‘depth exploration.
- **Filesystem**: secure file operations with configurable access controls.
- **GitHub**: tools to read, search, and manipulate Git repositories.
- **Prompts**: search for papers, extract and organize information, then produce comprehensive summaries.
- **Resources**: access to `papers://folders` (papers retrieved from arXiv).

### ğŸ§° Resources & Prompting (LvL4)

- **Resources (from **``**)**
  - `papers://folders` â†’ list available topics
  - `papers://{topic}` â†’ details of papers for a topic
- **Chat commands**
  - `@folders` â†’ show available folders
  - `@{topic}` â†’ show papers of a given topic
  - `/prompts` â†’ list available prompts
  - `/prompt <name> arg1=val1 â€¦` â†’ execute a prompt, then continue the conversation

## âš™ï¸ How It Works â€” by Level

**LvL1 â€” Local tools (no MCP)**

- Tools: `search_papers(topic, max_results)`, `extract_info(paper_id)`
- Flow: the LLM selects a tool â†’ the local function runs â†’ results are returned to the model â†’ final answer is generated.

**LvL2 â€” One MCP server (research)**

- The client starts a local MCP server (arXiv tools) over stdio.
- The LLM calls tools exposed by the server via MCP.

**LvL3 â€” Multiâ€‘MCP**

- Connects to multiple servers: filesystem, fetch, Git, research.
- Maps each tool name to the corresponding server session.

**LvL4 â€” MCP Prompts & Resources**

- Adds prompt listing/usage and resource listing/usage to the above.

## ğŸš€ Getting Started

### ğŸ§° Prerequisites

- Python **3.13.2**
- Install required libraries:

```bash
pip install -r requirements.txt
```

### ğŸ› ï¸ Installation

1. Clone the repository

```bash
git clone https://github.com/Bendrox/MCP_chatbot.git
cd MCP_chatbot
```

2. Install dependencies

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Usage

*In progress*

## ğŸ§± Components

*In progress*

## ğŸ”— Compatibility

- The OpenAI API can connect to MCP servers as tools (`"type": "mcp"`) in `responses.create` calls **only for remote servers** (HTTP/SSE) via `server_url`; **stdio is not supported** there.
- For **local servers (stdio)** such as filesystem, research, Git, use the **Agents SDK**, which can launch processes with `command`/`args`.

## ğŸ—ºï¸ Roadmap / Toâ€‘Do

- Enhance code and integrations.
- Unify output folders (`retreived_arxiv_papers` vs `papers`) or document the mapping.
- Add automated tests and linting.
- Provide an initialization script (make/uv/poe) + export examples.
- Make the MCP config path configurable (`MCP_SERVER_CONFIG` env, CLI flag, or class variable).
- Adjust OpenAI model names to the versions available at runtime.
- Explore related topics: OpenAI Agents SDK, Microsoft Learn MCP Server, Azure AI Foundry Agent Service.

## âœï¸ Author

OB

