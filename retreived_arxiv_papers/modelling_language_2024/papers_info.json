{
  "2408.10441v1": {
    "title": "Goldfish: Monolingual Language Models for 350 Languages",
    "authors": [
      "Tyler A. Chang",
      "Catherine Arnett",
      "Zhuowen Tu",
      "Benjamin K. Bergen"
    ],
    "summary": "For many low-resource languages, the only available language models are large\nmultilingual models trained on many languages simultaneously. However, using\nFLORES perplexity as a metric, we find that these models perform worse than\nbigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM\n7.1B). To facilitate research that focuses on low-resource languages, we\npre-train and release Goldfish, a suite of monolingual autoregressive\nTransformer language models up to 125M parameters for 350 languages. The\nGoldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98\nof 204 FLORES languages, despite each Goldfish model being over 10x smaller.\nHowever, the Goldfish significantly underperform larger multilingual models on\nreasoning benchmarks, suggesting that for low-resource languages,\nmultilinguality primarily improves general reasoning abilities rather than\nbasic text generation. We release models trained on 5MB (350 languages), 10MB\n(288 languages), 100MB (166 languages), and 1GB (83 languages) of text data\nwhere available. The Goldfish models are available as baselines, fine-tuning\nsources, or augmentations to existing models in low-resource NLP research, and\nthey are further useful for crosslinguistic studies requiring maximally\ncomparable models across languages.",
    "pdf_url": "http://arxiv.org/pdf/2408.10441v1",
    "published": "2024-08-19"
  },
  "2402.15511v1": {
    "title": "Sixth International Workshop on Languages for Modelling Variability (MODEVAR 2024)",
    "authors": [
      "Jessie Galasso-Carbonnel",
      "Chico Sundermann"
    ],
    "summary": "This is the proceedings of the Sixth International Workshop on Languages for\nModelling Variability (MODEVAR 2024) which was held at Bern, Switzerland,\nFebruary 06th 2024.",
    "pdf_url": "http://arxiv.org/pdf/2402.15511v1",
    "published": "2023-12-19"
  },
  "2402.14408v1": {
    "title": "Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching",
    "authors": [
      "Piotr Rybak"
    ],
    "summary": "Pre-trained language models have revolutionized the natural language\nunderstanding landscape, most notably BERT (Bidirectional Encoder\nRepresentations from Transformers). However, a significant challenge remains\nfor low-resource languages, where limited data hinders the effective training\nof such models. This work presents a novel approach to bridge this gap by\ntransferring BERT capabilities from high-resource to low-resource languages\nusing vocabulary matching. We conduct experiments on the Silesian and Kashubian\nlanguages and demonstrate the effectiveness of our approach to improve the\nperformance of BERT models even when the target language has minimal training\ndata. Our results highlight the potential of the proposed technique to\neffectively train BERT models for low-resource languages, thus democratizing\naccess to advanced language understanding models.",
    "pdf_url": "http://arxiv.org/pdf/2402.14408v1",
    "published": "2024-02-22"
  },
  "2405.04515v2": {
    "title": "A Transformer with Stack Attention",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "summary": "Natural languages are believed to be (mildly) context-sensitive. Despite\nunderpinning remarkably capable large language models, transformers are unable\nto model many context-free language tasks. In an attempt to address this\nlimitation in the modeling power of transformer-based language models, we\npropose augmenting them with a differentiable, stack-based attention mechanism.\nOur stack-based attention mechanism can be incorporated into any\ntransformer-based language model and adds a level of interpretability to the\nmodel. We show that the addition of our stack-based attention mechanism enables\nthe transformer to model some, but not all, deterministic context-free\nlanguages.",
    "pdf_url": "http://arxiv.org/pdf/2405.04515v2",
    "published": "2024-05-07"
  },
  "2402.14379v2": {
    "title": "Novi jezi\u010dki modeli za srpski jezik",
    "authors": [
      "Mihailo \u0160kori\u0107"
    ],
    "summary": "The paper will briefly present the development history of transformer-based\nlanguage models for the Serbian language. Several new models for text\ngeneration and vectorization, trained on the resources of the Society for\nLanguage Resources and Technologies, will also be presented. Ten selected\nvectorization models for Serbian, including two new ones, will be compared on\nfour natural language processing tasks. Paper will analyze which models are the\nbest for each selected task, how does their size and the size of their training\nsets affect the performance on those tasks, and what is the optimal setting to\ntrain the best language models for the Serbian language.",
    "pdf_url": "http://arxiv.org/pdf/2402.14379v2",
    "published": "2024-02-22"
  }
}